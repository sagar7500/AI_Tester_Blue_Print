# ğŸš€ Local Test Case Generator (B.L.A.S.T. Protocol)

An agentic AI tool that generates comprehensive test cases from User Stories using a **Local LLM** (Llama 3.2) via Ollama. Built with privacy and determinism in mind, ensuring no data leaves your machine.

## ğŸ—ï¸ Architecture

The system follows a strict 3-layer architecture to separate concerns and ensure reliability.

```mermaid
flowchart LR
    subgraph Client ["ğŸ–¥ï¸ Layer 4: Client"]
        User[User] <--> |Input/View| UI[Frontend UI]
    end

    subgraph Server ["âš™ï¸ Layer 2: Navigation"]
        UI <--> |HTTP POST /api/generate| API[FastAPI Backend]
    end

    subgraph Core ["ğŸ› ï¸ Layer 3: Tools"]
        API --> |Executes| Tool[Generator Tool]
        Tool --> |Validates| Schema[JSON Schema]
    end

    subgraph AI ["ğŸ§  Inference Engine"]
        Tool <--> |Generate| Ollama[(Ollama Service)]
        Ollama -.-> |Runs| Model[Llama 3.2]
    end

    style Client fill:#e1f5fe,stroke:#01579b
    style Server fill:#fff3e0,stroke:#e65100
    style Core fill:#f3e5f5,stroke:#4a148c
    style AI fill:#e8f5e9,stroke:#1b5e20
```

## âœ¨ Features

*   **ğŸ”’ Local & Private**: All processing happens on your machine using Ollama.
*   **âš¡ Real-time UI**: Dark-themed, chat-like interface for easy interaction.
*   **ğŸ“ Structured Output**: Strictly formatted JSON test cases with Positive, Negative, and Edge case categorization.
*   **ğŸ› ï¸ Deterministic Tooling**: Python-based tool logic ensures consistent prompting and error handling.
*   **ğŸŒ Single server**: One process serves the app and API â€” just open **http://localhost:3000**.

## ğŸ“‹ Prerequisites

*   [Ollama](https://ollama.com/) installed and running.
*   Python 3.10+
*   **Model**: `llama3.2` pulled (`ollama pull llama3.2`).

## ğŸš€ Quick Start

### Option A: Use http://localhost:3000 without running anything each time (Windows)

1. **One-time setup:** Double-click **`install_autostart.bat`**.
2. The server is added to **Windows Startup** and starts in the background (no window). It also starts immediately so you can use the app right away.
3. From now on, whenever you log in, the server will be running. Just open **http://localhost:3000** in your browser anytime â€” no need to run `start.bat`.

To stop the server from starting at login later, run **`uninstall_autostart.bat`**. To stop the server right now: open Task Manager, find **python.exe** or **uvicorn**, and end the task.

### Option B: Start the server only when you need it

* **Windows:** Double-click **`start.bat`** (opens a terminal and your browser). Close the terminal to stop the server.
* **Linux / Mac:** Run **`./start_system.sh`**, then open **http://localhost:3000**.

Use the app at **http://localhost:3000** â€” enter a feature or scenario and get structured test cases.

## ğŸ“‚ Project Structure

```
â”œâ”€â”€ architecture/       # Layer 1: SOPs and Architecture definitions
â”œâ”€â”€ backend/            # Layer 2: FastAPI app (serves API + frontend on :3000)
â”œâ”€â”€ frontend/           # Layer 4: UI assets (HTML, CSS, JS)
â”œâ”€â”€ tools/              # Layer 3: Deterministic Python Tools
â”œâ”€â”€ install_autostart.bat   # (Windows) Run once â€” server starts at login, use localhost:3000 anytime
â”œâ”€â”€ uninstall_autostart.bat # (Windows) Remove server from startup
â”œâ”€â”€ run_server.vbs         # (Windows) Runs server in background (used by autostart)
â”œâ”€â”€ start.bat              # Start server (Windows) â€” when you don't use autostart
â”œâ”€â”€ start_system.sh        # Start server (Linux/Mac)
â”œâ”€â”€ run_ollama.bat         # Optional: start Ollama
â”œâ”€â”€ Blast.md            # Master System Prompt & Protocol
â”œâ”€â”€ gemini.md           # Project Constitution (Data Schemas)
â””â”€â”€ task_plan.md        # Execution Plan
```

## ğŸ›¡ï¸ License

This project is part of the **AI Tester Blueprint** series.
